{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e88OLey6Bgd"
   },
   "source": [
    "# **Image Classification with Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsTzc3oi726u"
   },
   "source": [
    "# **Building a Convolutional Neural Network with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT2H7gBmACML"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "OCglDGFy8AhT",
    "outputId": "ebd8e8a1-250e-44d5-bd08-8a165139a39a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "import os,sys,humanize,psutil,GPUtil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "import imageio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwMflxlbAsfK"
   },
   "source": [
    "**Memory Utilization Checkup (Before/After Model Execution)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2bPbY1LEG_0",
    "outputId": "fb5e5f78-f535-4f74-915e-bf25d7e1b7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/ines/anaconda3/envs/jyp/lib/python3.9/site-packages (5.9.0)\n",
      "Requirement already satisfied: GPUtil in /home/ines/anaconda3/envs/jyp/lib/python3.9/site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "#CPU AND GPU LIBRARIES INSTALLATION (if you find error on import of psutil and GPUtil, please refer below)\n",
    "!pip install psutil\n",
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwrm76EmJ2Fv",
    "outputId": "2c6abe31-7da2-49b3-f888-d0bfc63f886b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       16308040 kB\n",
      "MemFree:         7672732 kB\n",
      "MemAvailable:   10241100 kB\n",
      "Buffers:          200976 kB\n",
      "Cached:          2632248 kB\n",
      "SwapCached:            0 kB\n",
      "Active:          1168216 kB\n",
      "Inactive:        6674948 kB\n",
      "Active(anon):       2820 kB\n",
      "Inactive(anon):  5150240 kB\n",
      "Active(file):    1165396 kB\n",
      "Inactive(file):  1524708 kB\n",
      "Unevictable:          16 kB\n",
      "Mlocked:              16 kB\n",
      "SwapTotal:       2097148 kB\n",
      "SwapFree:        2097148 kB\n",
      "Dirty:              2148 kB\n",
      "Writeback:             0 kB\n",
      "AnonPages:       5009772 kB\n",
      "Mapped:          1210652 kB\n",
      "Shmem:            143120 kB\n",
      "KReclaimable:     218720 kB\n",
      "Slab:             371956 kB\n",
      "SReclaimable:     218720 kB\n",
      "SUnreclaim:       153236 kB\n",
      "KernelStack:       19552 kB\n",
      "PageTables:        58976 kB\n",
      "NFS_Unstable:          0 kB\n",
      "Bounce:                0 kB\n",
      "WritebackTmp:          0 kB\n",
      "CommitLimit:    10251168 kB\n",
      "Committed_AS:   19697428 kB\n",
      "VmallocTotal:   34359738367 kB\n",
      "VmallocUsed:      106388 kB\n",
      "VmallocChunk:          0 kB\n",
      "Percpu:             6720 kB\n",
      "HardwareCorrupted:     0 kB\n",
      "AnonHugePages:         0 kB\n",
      "ShmemHugePages:        0 kB\n",
      "ShmemPmdMapped:        0 kB\n",
      "FileHugePages:         0 kB\n",
      "FilePmdMapped:         0 kB\n",
      "HugePages_Total:       0\n",
      "HugePages_Free:        0\n",
      "HugePages_Rsvd:        0\n",
      "HugePages_Surp:        0\n",
      "Hugepagesize:       2048 kB\n",
      "Hugetlb:               0 kB\n",
      "DirectMap4k:      644792 kB\n",
      "DirectMap2M:    10825728 kB\n",
      "DirectMap1G:     5242880 kB\n"
     ]
    }
   ],
   "source": [
    "#CPU MEMORY UTILIZATION\n",
    "\n",
    "!cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pgr9seSLdYf",
    "outputId": "098c34a9-beff-4d3a-b2ec-11c412c15673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       16308040 kB\n"
     ]
    }
   ],
   "source": [
    "!grep MemTotal /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLQu663TLjBQ",
    "outputId": "1b2b200d-94fb-4838-f688-96d2f5acc765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB]\n",
      "11264 MiB\n"
     ]
    }
   ],
   "source": [
    "#GPU MEMORY UTILIZATION \n",
    "!nvidia-smi --query-gpu=memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd_O6FIRJGtv",
    "outputId": "8ee3632f-aee1-4634-f181-48bab157cc53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU RAM Free: 10.5 GB\n",
      "GPU 0 ... Mem Free: 8918MB / 11264MB | Utilization  19%\n"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def mem_report():\n",
    "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
    "  \n",
    "  GPUs = GPUtil.getGPUs()\n",
    "  for i, gpu in enumerate(GPUs):\n",
    "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOyPVde22QGD"
   },
   "source": [
    "## **Let's Start with Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf7eW3HC9XVL"
   },
   "source": [
    "**Data Acquisition and ImageDataGenerator**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('/home/ines/code/ptb-xl/X_train/')\n",
    "test_dir = os.path.join('/home/ines/code/ptb-xl/X_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for images, labels in train_dl:\n",
    "    print(labels)\n",
    "    a += 1\n",
    "    if a == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3yhwpN_-BaS"
   },
   "source": [
    "**Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n",
      "torch.Size([32, 3, 64, 64])\n",
      "torch.Size([32, 3, 64, 64])\n",
      "torch.Size([32, 3, 64, 64])\n",
      "torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for images, labels in train_dl:\n",
    "    print(images.shape)\n",
    "    a += 1\n",
    "    if a == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense1): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (dense2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class cnn2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3,3), bias=True)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3,3), bias=True)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3,3), bias=True)\n",
    "        self.conv4 = nn.Conv2d(256, 64, kernel_size=(3,3), bias=True)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.maxpool2d = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dense1 = nn.Linear(64*2*2, 8, bias=True)\n",
    "        self.dense2 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool2d(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.maxpool2d(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = self.maxpool2d(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.maxpool2d(F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        output = torch.sigmoid(self.dense2(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = cnn2d()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FSzxq4KDtKN6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = tf.keras.models.Sequential([\\n                                    \\n    # First convolution layer \\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(64, 64, 3),use_bias=True),\\n    tf.keras.layers.MaxPooling2D(2, 2),\\n    tf.keras.layers.Dropout(0.25),\\n\\n    # Second convolution layer \\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu',use_bias=True),\\n    tf.keras.layers.MaxPooling2D(2,2),\\n    \\n\\n    # Third convolution layer \\n    tf.keras.layers.Conv2D(256, (3,3), activation='relu',use_bias=True),\\n    tf.keras.layers.MaxPooling2D(2,2),\\n    tf.keras.layers.Dropout(0.25),\\n\\n    # Fourth convolution layer  \\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',use_bias=True),\\n    tf.keras.layers.MaxPooling2D(2,2),\\n    tf.keras.layers.Dropout(0.25),\\n\\n    # Flatten the pooled feature maps\\n    tf.keras.layers.Flatten(),\\n\\n    # Fully connected hidden layer\\n    tf.keras.layers.Dense(8, activation='relu',use_bias=True),\\n\\n    # Output layer\\n    tf.keras.layers.Dense(1, activation='sigmoid',activity_regularizer=regularizers.L1(0.001))   \\n\\n])\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    \n",
    "    # First convolution layer \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(64, 64, 3),use_bias=True),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "    # Second convolution layer \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu',use_bias=True),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "\n",
    "    # Third convolution layer \n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu',use_bias=True),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "    # Fourth convolution layer  \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu',use_bias=True),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "    # Flatten the pooled feature maps\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Fully connected hidden layer\n",
    "    tf.keras.layers.Dense(8, activation='relu',use_bias=True),\n",
    "\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid',activity_regularizer=regularizers.L1(0.001))   \n",
    "\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVYZ8VwKBQRC"
   },
   "source": [
    "**Optimizer Implementation and model training/validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device_name)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "model.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RBN3GBMItPMH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss  0.0656 Accuracy  97.72%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 2: loss  0.0187 Accuracy  97.75%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 3: loss  0.0118 Accuracy  97.92%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 4: loss  0.0140 Accuracy  97.83%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 5: loss  0.0798 Accuracy  97.85%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 6: loss  0.0688 Accuracy  97.85%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 7: loss  0.0229 Accuracy  97.87%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 8: loss  0.1112 Accuracy  97.87%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 9: loss  0.2399 Accuracy  98.05%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 10: loss  0.0974 Accuracy  98.07%\n",
      "Val_Accuracy  97.62%\n",
      "Epoch 11: loss  0.0202 Accuracy  98.13%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 12: loss  0.0387 Accuracy  98.14%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 13: loss  0.0191 Accuracy  98.17%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 14: loss  0.0375 Accuracy  98.05%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 15: loss  0.0762 Accuracy  98.05%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 16: loss  0.0534 Accuracy  98.29%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 17: loss  0.0154 Accuracy  98.13%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 18: loss  0.0206 Accuracy  98.13%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 19: loss  0.1071 Accuracy  98.29%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 20: loss  0.1572 Accuracy  98.33%\n",
      "Val_Accuracy  96.46%\n",
      "Epoch 21: loss  0.0757 Accuracy  98.20%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 22: loss  0.0051 Accuracy  98.16%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 23: loss  0.0163 Accuracy  98.40%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 24: loss  0.0023 Accuracy  98.33%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 25: loss  0.0193 Accuracy  98.34%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 26: loss  0.0644 Accuracy  98.39%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 27: loss  0.0014 Accuracy  98.25%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 28: loss  0.0104 Accuracy  98.38%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 29: loss  0.0514 Accuracy  98.52%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 30: loss  0.1270 Accuracy  98.38%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 31: loss  0.1304 Accuracy  98.39%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 32: loss  0.0009 Accuracy  98.58%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 33: loss  0.2388 Accuracy  98.76%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 34: loss  0.0103 Accuracy  98.34%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 35: loss  0.1484 Accuracy  98.50%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 36: loss  0.0081 Accuracy  98.52%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 37: loss  0.0055 Accuracy  98.61%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 38: loss  0.1140 Accuracy  98.56%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 39: loss  0.0178 Accuracy  98.59%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 40: loss  0.1219 Accuracy  98.69%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 41: loss  0.0023 Accuracy  98.67%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 42: loss  0.0078 Accuracy  98.47%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 43: loss  0.0406 Accuracy  98.76%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 44: loss  0.0100 Accuracy  98.85%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 45: loss  0.0055 Accuracy  98.63%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 46: loss  0.1267 Accuracy  98.63%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 47: loss  0.0905 Accuracy  98.61%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 48: loss  0.0170 Accuracy  98.82%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 49: loss  0.0308 Accuracy  98.85%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 50: loss  0.0045 Accuracy  98.75%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 51: loss  0.0078 Accuracy  98.48%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 52: loss  0.0024 Accuracy  98.91%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 53: loss  0.0487 Accuracy  98.78%\n",
      "Val_Accuracy  96.63%\n",
      "Epoch 54: loss  0.0203 Accuracy  98.75%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 55: loss  0.0066 Accuracy  98.94%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 56: loss  0.0239 Accuracy  98.94%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 57: loss  0.0064 Accuracy  98.61%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 58: loss  0.0101 Accuracy  98.74%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 59: loss  0.0104 Accuracy  98.89%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 60: loss  0.0089 Accuracy  99.00%\n",
      "Val_Accuracy  96.67%\n",
      "Epoch 61: loss  0.1612 Accuracy  98.84%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 62: loss  0.0011 Accuracy  98.80%\n",
      "Val_Accuracy  97.66%\n",
      "Epoch 63: loss  0.0005 Accuracy  98.99%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 64: loss  0.0045 Accuracy  98.87%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 65: loss  0.0005 Accuracy  99.01%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 66: loss  0.1279 Accuracy  98.86%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 67: loss  0.0320 Accuracy  98.93%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 68: loss  0.0242 Accuracy  98.89%\n",
      "Val_Accuracy  96.59%\n",
      "Epoch 69: loss  0.0629 Accuracy  98.80%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 70: loss  0.0024 Accuracy  99.05%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 71: loss  0.0317 Accuracy  98.96%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 72: loss  0.0707 Accuracy  98.95%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 73: loss  0.0014 Accuracy  98.97%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 74: loss  0.0777 Accuracy  98.99%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 75: loss  0.0027 Accuracy  98.95%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 76: loss  0.0002 Accuracy  98.94%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 77: loss  0.0005 Accuracy  99.05%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 78: loss  0.0035 Accuracy  98.94%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 79: loss  0.0134 Accuracy  98.98%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 80: loss  0.0062 Accuracy  99.22%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 81: loss  0.0401 Accuracy  99.03%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 82: loss  0.0055 Accuracy  99.10%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 83: loss  0.0428 Accuracy  99.13%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 84: loss  0.1192 Accuracy  99.17%\n",
      "Val_Accuracy  97.62%\n",
      "Epoch 85: loss  0.0043 Accuracy  99.09%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 86: loss  0.0066 Accuracy  99.06%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 87: loss  0.0017 Accuracy  99.09%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 88: loss  0.0205 Accuracy  99.15%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 89: loss  0.0007 Accuracy  99.17%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 90: loss  0.0004 Accuracy  99.20%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 91: loss  0.0197 Accuracy  99.13%\n",
      "Val_Accuracy  97.62%\n",
      "Epoch 92: loss  0.0059 Accuracy  99.17%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 93: loss  0.0093 Accuracy  99.16%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 94: loss  0.0181 Accuracy  99.14%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 95: loss  0.0041 Accuracy  99.04%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 96: loss  0.0054 Accuracy  99.24%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 97: loss  0.0480 Accuracy  99.23%\n",
      "Val_Accuracy  96.50%\n",
      "Epoch 98: loss  0.0955 Accuracy  99.21%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 99: loss  0.0004 Accuracy  99.17%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 100: loss  0.0012 Accuracy  99.20%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 101: loss  0.0312 Accuracy  99.17%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 102: loss  0.0029 Accuracy  99.19%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 103: loss  0.0815 Accuracy  99.23%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 104: loss  0.0084 Accuracy  99.27%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 105: loss  0.0128 Accuracy  99.22%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 106: loss  0.0510 Accuracy  99.13%\n",
      "Val_Accuracy  96.63%\n",
      "Epoch 107: loss  0.0057 Accuracy  99.31%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 108: loss  0.0069 Accuracy  99.23%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 109: loss  0.1637 Accuracy  99.11%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 110: loss  0.0000 Accuracy  99.27%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 111: loss  0.0542 Accuracy  99.32%\n",
      "Val_Accuracy  96.67%\n",
      "Epoch 112: loss  0.0018 Accuracy  99.24%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 113: loss  0.0056 Accuracy  99.13%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 114: loss  0.0018 Accuracy  99.22%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 115: loss  0.0002 Accuracy  99.26%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 116: loss  0.0024 Accuracy  99.31%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 117: loss  0.0059 Accuracy  99.34%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 118: loss  0.0011 Accuracy  99.29%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 119: loss  0.0037 Accuracy  99.37%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 120: loss  0.0012 Accuracy  99.32%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 121: loss  0.0310 Accuracy  99.26%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 122: loss  0.0009 Accuracy  99.23%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 123: loss  0.0006 Accuracy  99.32%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 124: loss  0.0113 Accuracy  99.20%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 125: loss  0.0012 Accuracy  99.31%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 126: loss  0.0035 Accuracy  99.35%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 127: loss  0.0011 Accuracy  99.31%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 128: loss  0.0020 Accuracy  99.25%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 129: loss  0.0383 Accuracy  99.35%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 130: loss  0.0687 Accuracy  99.40%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 131: loss  0.0117 Accuracy  99.35%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 132: loss  0.0016 Accuracy  99.26%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 133: loss  0.0008 Accuracy  99.31%\n",
      "Val_Accuracy  96.63%\n",
      "Epoch 134: loss  0.0004 Accuracy  99.38%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 135: loss  0.1260 Accuracy  99.36%\n",
      "Val_Accuracy  96.63%\n",
      "Epoch 136: loss  0.0309 Accuracy  99.33%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 137: loss  0.0024 Accuracy  99.40%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 138: loss  0.0022 Accuracy  99.47%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 139: loss  0.0115 Accuracy  99.32%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 140: loss  0.0010 Accuracy  99.32%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 141: loss  0.0417 Accuracy  99.35%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 142: loss  0.0182 Accuracy  99.39%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 143: loss  0.0069 Accuracy  99.45%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 144: loss  0.1079 Accuracy  99.44%\n",
      "Val_Accuracy  96.46%\n",
      "Epoch 145: loss  0.0015 Accuracy  99.22%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 146: loss  0.0054 Accuracy  99.32%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 147: loss  0.0023 Accuracy  99.50%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 148: loss  0.0006 Accuracy  99.49%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 149: loss  0.0005 Accuracy  99.41%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 150: loss  0.0003 Accuracy  99.38%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 151: loss  0.0002 Accuracy  99.35%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 152: loss  0.0001 Accuracy  99.30%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 153: loss  0.0001 Accuracy  99.40%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 154: loss  0.0000 Accuracy  99.47%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 155: loss  0.0042 Accuracy  99.55%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 156: loss  0.0599 Accuracy  99.31%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 157: loss  0.0384 Accuracy  99.35%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 158: loss  0.0007 Accuracy  99.37%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 159: loss  0.0000 Accuracy  99.42%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 160: loss  0.0001 Accuracy  99.43%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 161: loss  0.0375 Accuracy  99.49%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 162: loss  0.0008 Accuracy  99.47%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 163: loss  0.0003 Accuracy  99.25%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 164: loss  0.0000 Accuracy  99.46%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 165: loss  0.0000 Accuracy  99.42%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 166: loss  0.0015 Accuracy  99.28%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 167: loss  0.0443 Accuracy  99.52%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 168: loss  0.0001 Accuracy  99.42%\n",
      "Val_Accuracy  96.67%\n",
      "Epoch 169: loss  0.0016 Accuracy  99.41%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 170: loss  0.0061 Accuracy  99.51%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 171: loss  0.0001 Accuracy  99.36%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 172: loss  0.0037 Accuracy  99.37%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 173: loss  0.0134 Accuracy  99.33%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 174: loss  0.0425 Accuracy  99.52%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 175: loss  0.0001 Accuracy  99.44%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 176: loss  0.0106 Accuracy  99.42%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 177: loss  0.0760 Accuracy  99.47%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 178: loss  0.0299 Accuracy  99.45%\n",
      "Val_Accuracy  96.46%\n",
      "Epoch 179: loss  0.0053 Accuracy  99.47%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 180: loss  0.0008 Accuracy  99.48%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 181: loss  0.0137 Accuracy  99.52%\n",
      "Val_Accuracy  96.55%\n",
      "Epoch 182: loss  0.0024 Accuracy  99.40%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 183: loss  0.0000 Accuracy  99.37%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 184: loss  0.0022 Accuracy  99.48%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 185: loss  0.0001 Accuracy  99.37%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 186: loss  0.0013 Accuracy  99.48%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 187: loss  0.0010 Accuracy  99.50%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 188: loss  0.0002 Accuracy  99.43%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 189: loss  0.0209 Accuracy  99.54%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 190: loss  0.0009 Accuracy  99.54%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 191: loss  0.0041 Accuracy  99.50%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 192: loss  0.0009 Accuracy  99.48%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 193: loss  0.3074 Accuracy  99.36%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 194: loss  0.0044 Accuracy  99.47%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 195: loss  0.0001 Accuracy  99.50%\n",
      "Val_Accuracy  97.66%\n",
      "Epoch 196: loss  0.0059 Accuracy  99.45%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 197: loss  0.0180 Accuracy  99.48%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 198: loss  0.0055 Accuracy  99.52%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 199: loss  0.0004 Accuracy  99.56%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 200: loss  0.0003 Accuracy  99.49%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 201: loss  0.0004 Accuracy  99.47%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 202: loss  0.0055 Accuracy  99.50%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 203: loss  0.0002 Accuracy  99.39%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 204: loss  0.0071 Accuracy  99.54%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 205: loss  0.0007 Accuracy  99.50%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 206: loss  0.0136 Accuracy  99.47%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 207: loss  0.0000 Accuracy  99.41%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 208: loss  0.0000 Accuracy  99.47%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 209: loss  0.0013 Accuracy  99.54%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 210: loss  0.0002 Accuracy  99.48%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 211: loss  0.0143 Accuracy  99.55%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 212: loss  0.0001 Accuracy  99.59%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 213: loss  0.0168 Accuracy  99.42%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 214: loss  0.0000 Accuracy  99.54%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 215: loss  0.0003 Accuracy  99.46%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 216: loss  0.0080 Accuracy  99.48%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 217: loss  0.0001 Accuracy  99.58%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 218: loss  0.0000 Accuracy  99.57%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 219: loss  0.0000 Accuracy  99.57%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 220: loss  0.0000 Accuracy  99.54%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 221: loss  0.0003 Accuracy  99.48%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 222: loss  0.0182 Accuracy  99.56%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 223: loss  0.0120 Accuracy  99.53%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 224: loss  0.0111 Accuracy  99.56%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 225: loss  0.0011 Accuracy  99.50%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 226: loss  0.0014 Accuracy  99.53%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 227: loss  0.0370 Accuracy  99.53%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 228: loss  0.0022 Accuracy  99.54%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 229: loss  0.0001 Accuracy  99.43%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 230: loss  0.0024 Accuracy  99.53%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 231: loss  0.0006 Accuracy  99.51%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 232: loss  0.0015 Accuracy  99.58%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 233: loss  0.0945 Accuracy  99.64%\n",
      "Val_Accuracy  96.30%\n",
      "Epoch 234: loss  0.0027 Accuracy  99.34%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 235: loss  0.0202 Accuracy  99.56%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 236: loss  0.0001 Accuracy  99.48%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 237: loss  0.0050 Accuracy  99.60%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 238: loss  0.0001 Accuracy  99.56%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 239: loss  0.0006 Accuracy  99.56%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 240: loss  0.0045 Accuracy  99.50%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 241: loss  0.0002 Accuracy  99.56%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 242: loss  0.0023 Accuracy  99.54%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 243: loss  0.0000 Accuracy  99.58%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 244: loss  0.0060 Accuracy  99.61%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 245: loss  0.0002 Accuracy  99.48%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 246: loss  0.0000 Accuracy  99.62%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 247: loss  0.0011 Accuracy  99.50%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 248: loss  0.0020 Accuracy  99.45%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 249: loss  0.0004 Accuracy  99.55%\n",
      "Val_Accuracy  96.59%\n",
      "Epoch 250: loss  0.0542 Accuracy  99.49%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 251: loss  0.0152 Accuracy  99.55%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 252: loss  0.0023 Accuracy  99.60%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 253: loss  0.0013 Accuracy  99.58%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 254: loss  0.0001 Accuracy  99.49%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 255: loss  0.0025 Accuracy  99.66%\n",
      "Val_Accuracy  96.34%\n",
      "Epoch 256: loss  0.0471 Accuracy  99.51%\n",
      "Val_Accuracy  95.93%\n",
      "Epoch 257: loss  0.0008 Accuracy  99.57%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 258: loss  0.0016 Accuracy  99.57%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 259: loss  0.0040 Accuracy  99.58%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 260: loss  0.0017 Accuracy  99.62%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 261: loss  0.0083 Accuracy  99.51%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 262: loss  0.0016 Accuracy  99.52%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 263: loss  0.0011 Accuracy  99.53%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 264: loss  0.0008 Accuracy  99.50%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 265: loss  0.0008 Accuracy  99.49%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 266: loss  0.0002 Accuracy  99.63%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 267: loss  0.0763 Accuracy  99.64%\n",
      "Val_Accuracy  96.34%\n",
      "Epoch 268: loss  0.0005 Accuracy  99.62%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 269: loss  0.0063 Accuracy  99.67%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 270: loss  0.0000 Accuracy  99.60%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 271: loss  0.0000 Accuracy  99.46%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 272: loss  0.0000 Accuracy  99.54%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 273: loss  0.0000 Accuracy  99.69%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 274: loss  0.0011 Accuracy  99.40%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 275: loss  0.0008 Accuracy  99.61%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 276: loss  0.0004 Accuracy  99.59%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 277: loss  0.0000 Accuracy  99.60%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 278: loss  0.0003 Accuracy  99.50%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 279: loss  0.0008 Accuracy  99.55%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 280: loss  0.0182 Accuracy  99.64%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 281: loss  0.0256 Accuracy  99.57%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 282: loss  0.0006 Accuracy  99.54%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 283: loss  0.0234 Accuracy  99.62%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 284: loss  0.0002 Accuracy  99.52%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 285: loss  0.0569 Accuracy  99.65%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 286: loss  0.0003 Accuracy  99.62%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 287: loss  0.0032 Accuracy  99.61%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 288: loss  0.0026 Accuracy  99.60%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 289: loss  0.0015 Accuracy  99.67%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 290: loss  0.0015 Accuracy  99.58%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 291: loss  0.0004 Accuracy  99.56%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 292: loss  0.0002 Accuracy  99.60%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 293: loss  0.0005 Accuracy  99.59%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 294: loss  0.0365 Accuracy  99.50%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 295: loss  0.1872 Accuracy  99.57%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 296: loss  0.0002 Accuracy  99.59%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 297: loss  0.0000 Accuracy  99.64%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 298: loss  0.0006 Accuracy  99.56%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 299: loss  0.0007 Accuracy  99.69%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 300: loss  0.1042 Accuracy  99.54%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 301: loss  0.0000 Accuracy  99.58%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 302: loss  0.0002 Accuracy  99.63%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 303: loss  0.0000 Accuracy  99.59%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 304: loss  0.0001 Accuracy  99.54%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 305: loss  0.0009 Accuracy  99.70%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 306: loss  0.0201 Accuracy  99.65%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 307: loss  0.0003 Accuracy  99.52%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 308: loss  0.0002 Accuracy  99.39%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 309: loss  0.0007 Accuracy  99.75%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 310: loss  0.0000 Accuracy  99.62%\n",
      "Val_Accuracy  97.53%\n",
      "Epoch 311: loss  0.0000 Accuracy  99.71%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 312: loss  0.0000 Accuracy  99.56%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 313: loss  0.0113 Accuracy  99.54%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 314: loss  0.0002 Accuracy  99.54%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 315: loss  0.0006 Accuracy  99.51%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 316: loss  0.0000 Accuracy  99.64%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 317: loss  0.0510 Accuracy  99.73%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 318: loss  0.0012 Accuracy  99.61%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 319: loss  0.0004 Accuracy  99.61%\n",
      "Val_Accuracy  96.59%\n",
      "Epoch 320: loss  0.0021 Accuracy  99.70%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 321: loss  0.0005 Accuracy  99.64%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 322: loss  0.0093 Accuracy  99.65%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 323: loss  0.0000 Accuracy  99.57%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 324: loss  0.0007 Accuracy  99.58%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 325: loss  0.0000 Accuracy  99.65%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 326: loss  0.0004 Accuracy  99.70%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 327: loss  0.0001 Accuracy  99.56%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 328: loss  0.0000 Accuracy  99.64%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 329: loss  0.0000 Accuracy  99.68%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 330: loss  0.0020 Accuracy  99.63%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 331: loss  0.0490 Accuracy  99.69%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 332: loss  0.0016 Accuracy  99.52%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 333: loss  0.0012 Accuracy  99.59%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 334: loss  0.0012 Accuracy  99.63%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 335: loss  0.0000 Accuracy  99.51%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 336: loss  0.0014 Accuracy  99.59%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 337: loss  0.0011 Accuracy  99.58%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 338: loss  0.0006 Accuracy  99.60%\n",
      "Val_Accuracy  97.66%\n",
      "Epoch 339: loss  0.0105 Accuracy  99.63%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 340: loss  0.0004 Accuracy  99.66%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 341: loss  0.0005 Accuracy  99.56%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 342: loss  0.0013 Accuracy  99.56%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 343: loss  0.0053 Accuracy  99.65%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 344: loss  0.4674 Accuracy  99.67%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 345: loss  0.1460 Accuracy  99.59%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 346: loss  0.0006 Accuracy  99.54%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 347: loss  0.0016 Accuracy  99.59%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 348: loss  0.0001 Accuracy  99.67%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 349: loss  0.0004 Accuracy  99.65%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 350: loss  0.0000 Accuracy  99.70%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 351: loss  0.0000 Accuracy  99.66%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 352: loss  0.0066 Accuracy  99.56%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 353: loss  0.0858 Accuracy  99.66%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 354: loss  0.0005 Accuracy  99.61%\n",
      "Val_Accuracy  97.70%\n",
      "Epoch 355: loss  0.0001 Accuracy  99.71%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 356: loss  0.0728 Accuracy  99.55%\n",
      "Val_Accuracy  97.70%\n",
      "Epoch 357: loss  0.0000 Accuracy  99.68%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 358: loss  0.0009 Accuracy  99.63%\n",
      "Val_Accuracy  97.66%\n",
      "Epoch 359: loss  0.0403 Accuracy  99.56%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 360: loss  0.0010 Accuracy  99.66%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 361: loss  0.0000 Accuracy  99.75%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 362: loss  0.0037 Accuracy  99.68%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 363: loss  0.0020 Accuracy  99.66%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 364: loss  0.0144 Accuracy  99.55%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 365: loss  0.0110 Accuracy  99.74%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 366: loss  0.0006 Accuracy  99.78%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 367: loss  0.0000 Accuracy  99.66%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 368: loss  0.0136 Accuracy  99.64%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 369: loss  0.0012 Accuracy  99.70%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 370: loss  0.0000 Accuracy  99.60%\n",
      "Val_Accuracy  97.70%\n",
      "Epoch 371: loss  0.0068 Accuracy  99.68%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 372: loss  0.0075 Accuracy  99.63%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 373: loss  0.0001 Accuracy  99.60%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 374: loss  0.0000 Accuracy  99.47%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 375: loss  0.0000 Accuracy  99.60%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 376: loss  0.0060 Accuracy  99.81%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 377: loss  0.0587 Accuracy  99.76%\n",
      "Val_Accuracy  97.66%\n",
      "Epoch 378: loss  0.0002 Accuracy  99.60%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 379: loss  0.0061 Accuracy  99.74%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 380: loss  0.0067 Accuracy  99.57%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 381: loss  0.0000 Accuracy  99.62%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 382: loss  0.0000 Accuracy  99.59%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 383: loss  0.0000 Accuracy  99.36%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 384: loss  0.0003 Accuracy  99.72%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 385: loss  0.0001 Accuracy  99.73%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 386: loss  0.0049 Accuracy  99.68%\n",
      "Val_Accuracy  96.92%\n",
      "Epoch 387: loss  0.0000 Accuracy  99.62%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 388: loss  0.0006 Accuracy  99.70%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 389: loss  0.0001 Accuracy  99.69%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 390: loss  0.0010 Accuracy  99.64%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 391: loss  0.0010 Accuracy  99.62%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 392: loss  0.0072 Accuracy  99.49%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 393: loss  0.0306 Accuracy  99.49%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 394: loss  0.0003 Accuracy  99.53%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 395: loss  0.0000 Accuracy  99.62%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 396: loss  0.0284 Accuracy  99.66%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 397: loss  0.0044 Accuracy  99.71%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 398: loss  0.0349 Accuracy  99.63%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 399: loss  0.0000 Accuracy  99.77%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 400: loss  0.0002 Accuracy  99.65%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 401: loss  0.0001 Accuracy  99.59%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 402: loss  0.0485 Accuracy  99.60%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 403: loss  0.0000 Accuracy  99.68%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 404: loss  0.0000 Accuracy  99.58%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 405: loss  0.0037 Accuracy  99.60%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 406: loss  0.0000 Accuracy  99.74%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 407: loss  0.0001 Accuracy  99.41%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 408: loss  0.0394 Accuracy  99.61%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 409: loss  0.0054 Accuracy  99.68%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 410: loss  0.0001 Accuracy  99.63%\n",
      "Val_Accuracy  96.79%\n",
      "Epoch 411: loss  0.0000 Accuracy  99.54%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 412: loss  0.0002 Accuracy  99.73%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 413: loss  0.0012 Accuracy  99.57%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 414: loss  0.0018 Accuracy  99.67%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 415: loss  0.0006 Accuracy  99.63%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 416: loss  0.0146 Accuracy  99.66%\n",
      "Val_Accuracy  96.42%\n",
      "Epoch 417: loss  0.0353 Accuracy  99.69%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 418: loss  0.0005 Accuracy  99.64%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 419: loss  0.0804 Accuracy  99.48%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 420: loss  0.0001 Accuracy  99.59%\n",
      "Val_Accuracy  97.25%\n",
      "Epoch 421: loss  0.0394 Accuracy  99.61%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 422: loss  0.0000 Accuracy  99.68%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 423: loss  0.0000 Accuracy  99.48%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 424: loss  0.0000 Accuracy  99.67%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 425: loss  0.0001 Accuracy  99.76%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 426: loss  0.0000 Accuracy  99.69%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 427: loss  0.0660 Accuracy  99.64%\n",
      "Val_Accuracy  97.49%\n",
      "Epoch 428: loss  0.0935 Accuracy  99.71%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 429: loss  0.0089 Accuracy  99.58%\n",
      "Val_Accuracy  96.75%\n",
      "Epoch 430: loss  0.0341 Accuracy  99.46%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 431: loss  0.0002 Accuracy  99.77%\n",
      "Val_Accuracy  97.45%\n",
      "Epoch 432: loss  0.0000 Accuracy  99.72%\n",
      "Val_Accuracy  97.00%\n",
      "Epoch 433: loss  0.0291 Accuracy  99.72%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 434: loss  0.0027 Accuracy  99.57%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 435: loss  0.1445 Accuracy  99.70%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 436: loss  0.0085 Accuracy  99.46%\n",
      "Val_Accuracy  96.71%\n",
      "Epoch 437: loss  0.0005 Accuracy  99.70%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 438: loss  0.0004 Accuracy  99.59%\n",
      "Val_Accuracy  96.59%\n",
      "Epoch 439: loss  0.0009 Accuracy  99.59%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 440: loss  0.1027 Accuracy  99.57%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 441: loss  0.0000 Accuracy  99.67%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 442: loss  0.0000 Accuracy  99.54%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 443: loss  0.0000 Accuracy  99.36%\n",
      "Val_Accuracy  96.88%\n",
      "Epoch 444: loss  0.0025 Accuracy  99.56%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 445: loss  0.0000 Accuracy  99.63%\n",
      "Val_Accuracy  96.96%\n",
      "Epoch 446: loss  0.0004 Accuracy  99.46%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 447: loss  0.0003 Accuracy  99.62%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 448: loss  0.0000 Accuracy  99.73%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 449: loss  0.0147 Accuracy  99.34%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 450: loss  0.0003 Accuracy  99.62%\n",
      "Val_Accuracy  97.57%\n",
      "Epoch 451: loss  0.0000 Accuracy  99.68%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 452: loss  0.0000 Accuracy  99.74%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 453: loss  0.0000 Accuracy  99.65%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 454: loss  0.3105 Accuracy  99.65%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 455: loss  0.0014 Accuracy  99.38%\n",
      "Val_Accuracy  97.04%\n",
      "Epoch 456: loss  0.0142 Accuracy  99.68%\n",
      "Val_Accuracy  97.29%\n",
      "Epoch 457: loss  0.0033 Accuracy  99.43%\n",
      "Val_Accuracy  96.83%\n",
      "Epoch 458: loss  0.0124 Accuracy  99.60%\n",
      "Val_Accuracy  97.33%\n",
      "Epoch 459: loss  0.0000 Accuracy  99.60%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 460: loss  0.0059 Accuracy  99.62%\n",
      "Val_Accuracy  97.41%\n",
      "Epoch 461: loss  0.0000 Accuracy  99.72%\n",
      "Val_Accuracy  97.16%\n",
      "Epoch 462: loss  0.0082 Accuracy  99.74%\n",
      "Val_Accuracy  97.08%\n",
      "Epoch 463: loss  0.0001 Accuracy  99.62%\n",
      "Val_Accuracy  97.12%\n",
      "Epoch 464: loss  0.0001 Accuracy  99.67%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 465: loss  0.0000 Accuracy  99.65%\n",
      "Val_Accuracy  97.37%\n",
      "Epoch 466: loss  0.0096 Accuracy  99.66%\n",
      "Val_Accuracy  97.20%\n",
      "Epoch 467: loss  0.0008 Accuracy  99.47%\n",
      "Val_Accuracy  97.12%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dl):\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 435\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torch/utils/data/dataloader.py:475\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    474\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    477\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torchvision/datasets/folder.py:153\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    151\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torchvision/transforms/transforms.py:67\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 67\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torchvision/transforms/transforms.py:104\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jyp/lib/python3.9/site-packages/torchvision/transforms/functional.py:102\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    100\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_acc = 99.6\n",
    "epochs = 500\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "tr_acc = []\n",
    "valid_acc = []\n",
    "tr_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        predicted = (outputs >= 0.5).float()  # 0.5를 기준으로 클래스를 나눕니다\n",
    "        \n",
    "        correct = (predicted.squeeze(1) == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc = 100 * total_correct / total_samples\n",
    "    tr_acc.append(acc)\n",
    "    tr_loss.append(loss.item())\n",
    "    print(f'Epoch {epoch+1}: loss {loss: .4f}, Accuracy {acc: .2f}%')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_tot_correct = 0\n",
    "        val_tot_samples = 0\n",
    "        for images, labels in test_dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            val_outputs = model(images)\n",
    "\n",
    "            val_pred = (val_outputs >= 0.5).float()  # 0.5를 기준으로 클래스를 나눕니다\n",
    "\n",
    "            lab = labels.cpu().numpy()\n",
    "            y_pred.extend(val_pred)\n",
    "            y_true.extend(lab)\n",
    "            \n",
    "            val_cor = (val_pred.squeeze(1) == labels).sum().item()\n",
    "            val_tot_correct += val_cor\n",
    "            val_tot_samples += labels.size(0)\n",
    "\n",
    "            val_loss = loss_fn(val_outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "\n",
    "        val_acc = 100 * val_tot_correct / val_tot_samples\n",
    "        valid_acc.append(val_acc)\n",
    "        valid_loss.append(val_loss.item())\n",
    "        print(f'val_loss {val_loss: .4f}, Val_Accuracy {val_acc: .2f}%')\n",
    "\n",
    "        if val_acc >= target_acc:\n",
    "            print(f\"Reached target accuracy of {val_acc: .4f}, stop training\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "label_mapping = {'ASMI': 0, 'Normal': 1}\n",
    "\n",
    "y_pred_cpu = [pred.cpu().numpy() if isinstance(pred, torch.Tensor) else pred for pred in y_pred]\n",
    "y_true_cpu = [true.cpu().numpy() if isinstance(true, torch.Tensor) else true for true in y_true]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['ASMI', 'Normal'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Q0MPMwh3EgY"
   },
   "outputs": [],
   "source": [
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCdS_0U65A0-",
    "outputId": "9c98cd44-cb57-4102-d99f-d0157487053a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9789062738418579\n",
      "Validation Accuracy:  0.9906250238418579\n",
      "Validation Specificity:  1.0\n",
      "Validation Sensitivity:  1.0\n",
      "Validation Recall:  1.0\n",
      "Validation Precision:  0.9983713626861572\n",
      "Validation Loss:  0.04193344712257385\n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(1,epochs+1,1), tr_acc)\n",
    "plt.plot(range(1,epochs+1,1), valid_acc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,epochs+1,1), tr_loss)\n",
    "plt.plot(range(1,epochs+1,1), valid_loss)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7e88OLey6Bgd"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "jyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
